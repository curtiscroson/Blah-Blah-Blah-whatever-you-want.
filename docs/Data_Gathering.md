# Obtaining the Data
<small> A summary of how we extracted our data from the original source and how we cleaned and condensed it.


## Web Scapper

Our data comes from the national archives websites main page is located at aad.archives.gov. From the main page under the section labeled Genealogy/ Personal History we selected the section for Prisoners of War. And then when redirected to the next page where we were given in a list of record groups and the first one contains our data set for WW2 from 1942 to 1947. We then clicked to search in to that set. This link brings you to the description where we then had to click the search button again to be directed to the fielded search page for this data set. There are several options to sort the data and create a specified view of the data. The options for refining are: serial number, name, service, state of residence, area, status, detaining power, and camp. We did not use serial number, name, state of residence, camp, and area to sort and refine the data. We most utilized service code, status, and detaining power. There were 7 ways to navigate service code: Army, civilian, Marine, Merchant Marine, Navy, and war correspondent. Within status there were five categories to sort with: Died as Prisoner of War, Not Above Cases, Executed, died in Ship's Sinking or Result of Ship Sinking, shot While Attempting Escape, returned to Military Control, Liberated or Repatriated (this option is selectable 3 different times.) Detaining power only had two options and these are Japan and Germany.

![Image One](http://i.imgur.com/rBajquo.png)
Above: Total List of Selectors to grab the text data

Originally, we were going to try and code a scrapper for this data set. We decided to both work on independent scrape methods to obtain the data. Curtis worked on the code in python, while I worked with the chrome extension WebScrapper. We found Webscrapper to be more accessible and efficient for our project and we chose to move forward with this method. There were several scrapes we had to preform to get all the data that we concluded with. All the different scrapes utilized the same sitemap in WebScrapper. When the sitemap is originally run there must be a page load delay set because the archives records page has a redirect. We must allow enough time for the page with valid HTML to appear. 1.8-5 seconds is a sufficient delay to allow for the redirect. Once the page has loaded the scrape can begin. At the root level of the sitemap there are two selectors: Link and Next Page. When the initial page loads in to the ghost window the first selector, Link, collects the first 50 links for each individual record. Then the second selector, Next Page, navigates the phantom window to the next page where Link will collect the next 50 records’ URLs. We were unable to use the same Next Page selector for the 2nd or any subsequent pages because the HTML changes for that link. We had to create a new selector which we labeled Next Page 2; this selector functioned exactly like Next Page. For the system to run on a loop through all the pages Next Page 2 had to be made a parent of itself, and its other child was Link. From the 2nd page on the scrape could collect each record’s link autonomously.

![Image Two](http://i.imgur.com/dfC6znj.png)
![Image Three](http://i.imgur.com/WXszIkM.png)
Above The differences in HTML for each selector can be seen in the second column.

![Image Four](http://i.imgur.com/v8TANpo.png)
Above is the selector graph for the full information scrape

Once all the links were collected each individual record was scrapped of its data. To retrieve each, field a different selector was created, the following are the 24 sectors that a were created. POW Transport Ships, Rep, Camp, Detaining Power, Status, Source of Report, Latest Report Date: Year, Latest Report Date: Month, Latest Report Date: Day, Area, Parent Unit Type, Parent Unit Number, Type of Organization, State of Residence, Racial Group Code, Date Report: Year, Date Report: Month, Date Report: Day, Arm or Service Code, Arm or Service, Service Code, Grade Code, Grade, Alpha, and Name.

We preformed several scrapes to gain the data. The first few we ran were merely test and we only yielded a few hundred results each. There were two sets of data we could directly download a CSV for, these were the War Correspondents and Merchant Marines, their records were less than 1,000 which was the limit to directly download from them. On the 5th or 6th try we were finally able to get the scrape to run through several hundred pages of records and that scrape yielded nearly 12,000 results. The scrape itself took roughly 48 hours. Added with the other smaller scrapes we preformed we had roughly 20,000 results of data. But we wanted to be able to have it all.  We ran another scrape that lasted 4 consecutive days and nights. It collected over 75,000 URLS. When the scrape completed the data from those URLS was unable to be downloaded and we lost all that data. It was confirmed through Dr. David Thomas that there was not enough local memory to hold that much information and it caused the program to crash when complete.

We decided that obtaining all 143,000 records would consume too much time and hurtful to our project so we preformed one last scrape that was altered to answer a specific question. We wanted to understand the relationship between where and how individuals died in comparison to their race. This scrape was run with a reduced amount of text selectors. We only looked at ten for this scrape: Camp, Detaining Power, Status, Source of Report, Area, Type of Organization, Racial Group Code, Arm or Service, Grade, Alpha, and Name. This scrape was performed in 3 different sections with approximately 92 pages of 50 links each. The scrapes took 6 hours individually to complete each. At the end we had a complete set of data for all that had died as POWs, totaling about 15,000 records. These sets had some overlap in where they scrapped so when we combined them in to one excel file we needed to delete any duplicates and we were easily able to do this through excel. The “Data” tab in the workbook listed an option to delete duplicates. We had to select the columns we wished to match and delete duplicates for, in this case we wanted all fields to match. After we completed this our number of records dropped to 13,801 which was accurate to what the original data source provided as the total.

![Image Five](http://i.imgur.com/gAhhnMf.png)
Left is the selector graph for the specialized scrape


## Cleaning our Data

We used open refine to clean out data and make it more friendly for Tableau to understand. For each column, we used the text facet feature to preform mass edits in the program. The first step in cleaning our data was to remove in blanks that existed and make any irregularities unified. For any fields that appeared as blank or null we switched them to Unidentified Code. The field that we used as our main classification was Arm or Service, which should have yielded Army, Navy, Marine, Civilian, Merchant Marine or War Correspondent. The fields or Navy, Marine, Merchant Marine and War Correspondent all compiled correctly but Army and civilian did not transfer correctly for all files. For Army, their specific rank and job appeared in this field while for Civilian they were merely left blank. It was simple to replace all the blanks with Civilian but Army took slightly longer. There were roughly 30 different inputs for Army and each group had to be selected and redone as civilian. This process took roughly 30 minutes for each of our different data sets, we did a quick search on each field to ensure they were supposed to be Army and not a different branch. There was also a section called “Bureau of Yards and Docks” which upon a quick search revealed it was a navy branch and we switched that over. When we thought the fields, we would be using were cleaned and unified we opened the first data set in Tableau and found that the way the “Area” tab which signified where POWs were detained was not in a format that was recognizable. They each had a “Theatre” specified e.x. European Theatre: Poland. We could remove these two different ways. The first was to use the prior method of consolidating the fields but I found a faster way just using excel. You utilize the “Ctrl+F” feature and you type in the theatre you wish to remove. For the prior example you Would type: “European Theatre: “ When doing this you must be sure to add the space after the colon so it is removed as well. There is then a tab called “Replace” which you would click and type nothing in to the replace section. Then you select all options and confirm. This will select all that start with “European Theatre: “ This was helpful because there would be multiple countries listed after the colon, in open refine each of these countries would have to be done individually.

## Additional Data Created

After we completed all our scrapes we wanted to compile some totals that wouldn’t be properly reflected in our data set. One of the sets created was the total number of POWs detained by Germany and Japan. Another small individual data set created was the totals for each service type detained by each power. The last small data set we created was the total number of POWs in each country that contained camps. 
